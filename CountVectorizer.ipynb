{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CountVectorizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/IA376A/blob/master/CountVectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpHsn58xAL5J",
        "colab_type": "text"
      },
      "source": [
        "# Nome: Paulo Finardi\n",
        "\n",
        "\n",
        "Funções a serem implementadas:\n",
        "1. vocab = build_vocab(corpus)\n",
        "2. corpus_tok = tokenizer(corpus, vocab)\n",
        "3. doc_term = feature(corpus_tok)\n",
        "\n",
        "Enquanto está depurando o seu programa, utilize um corpus bem pequeno, com poucos exemplos e depois de depurado, rode ele nos 1000 exemplos do imdb_sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoJtQrmSQgoA",
        "colab_type": "text"
      },
      "source": [
        "# Bibliotecas utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHHuSVPKffKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from itertools import chain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThzSeNEKIm6J",
        "colab_type": "text"
      },
      "source": [
        "# Download do dataset do IMDB_sample (apenas 1000 exemplos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvMu6ILcKJEd",
        "colab_type": "text"
      },
      "source": [
        "O dataset está sendo carregado dos datasets disponibilizados pelo curso fast.ai: https://course.fast.ai/datasets.html\n",
        "\n",
        "O comando wget busca o arquivo imdb.tgz\n",
        "O comando tar descomprime o arquivo no diretório local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc80_T9eCfgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "53596152-eb29-4878-8647-71873a7e8f1b"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/examples/imdb_sample.tgz\n",
        "!tar -xzf imdb_sample.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-08 21:17:13--  http://files.fast.ai/data/examples/imdb_sample.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
            "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 571827 (558K) [application/x-gtar-compressed]\n",
            "Saving to: ‘imdb_sample.tgz’\n",
            "\n",
            "\rimdb_sample.tgz       0%[                    ]       0  --.-KB/s               \rimdb_sample.tgz      39%[======>             ] 219.16K   839KB/s               \rimdb_sample.tgz     100%[===================>] 558.42K  1.66MB/s    in 0.3s    \n",
            "\n",
            "2020-03-08 21:17:14 (1.66 MB/s) - ‘imdb_sample.tgz’ saved [571827/571827]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFf1MI9XRNdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "adf58aba-69df-44ef-84c0-a1cde84a7d37"
      },
      "source": [
        "df = pd.read_csv('imdb_sample/texts.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3-AbBtOP2nU",
        "colab_type": "text"
      },
      "source": [
        "# Minha Solução "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKfI-R3VFlDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text with some ramdom special chars %#$[] etc...\n",
        "my_text = [\"O valor-p, é a #prob. de se obter% uma estatística de teste igual ou \\\n",
        "           mais extrema que aquela observada em uma amostra, sob a hipótese nula.\",\n",
        "           \"Um valor-p pequeno significa que a prob. de obter um $valor da \\\n",
        "           estatística[] de teste como o observado é muito improvável, levando \\\n",
        "           assim à rejeição da hipótese nula.\"\n",
        "          ]     \n",
        "\n",
        "corpus = ['This is the first document.',\n",
        "          'This document is the second document.',\n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?',\n",
        "          ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PjBD6348pP-",
        "colab_type": "text"
      },
      "source": [
        "# Fç build_vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jxl4dIL_4JL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18c2b432-e9de-4f26-d955-f11794f62a40"
      },
      "source": [
        "def build_vocab(allsentences):\n",
        "  words = []\n",
        "  for sentence in allsentences:\n",
        "    w = text_cleaner(sentence)\n",
        "    words.extend(w)\n",
        "  o = sorted(list(set(words)))\n",
        "  return o\n",
        "\n",
        "def text_cleaner(sentence):\n",
        "  words = re.sub(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ:-]\", \" \",  str(sentence))\n",
        "  words = re.sub('\\s\\s+', ' ',words).split() # drop double spaces\n",
        "  cleaned_text = [w.lower() for w in words] \n",
        "  return cleaned_text \n",
        "\n",
        "##########\n",
        "# Testing\n",
        "##########\n",
        " \n",
        "vocab = build_vocab(corpus)\n",
        "print(f'My solution: {vocab}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My solution: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05r-GuJ38pG1",
        "colab_type": "text"
      },
      "source": [
        "## Comparando com o Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjGmRIwe8o-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2de3799b-8e7b-488c-bcfe-b6d37d45aa95"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X  = cv.fit_transform(corpus)\n",
        "names = cv.get_feature_names()\n",
        "print(f'SKLearn: {names}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SKLearn: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z21AX9w9iHG",
        "colab_type": "text"
      },
      "source": [
        "# My tokenizer class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImIKp2Ed90WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My tokenizer class\n",
        "class Vocab:\n",
        "\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.token2index   = {}\n",
        "    self.token2count   = {}\n",
        "    self.index2token   = {}\n",
        "    self.num_tokens    = 0  \n",
        "    self.num_chunks    = 0\n",
        "    self.longest_chunk = 0\n",
        "  \n",
        "  def add_token(self, token):\n",
        "    if token not in self.token2index:\n",
        "      self.token2index[token] = self.num_tokens\n",
        "      self.token2count[token] = 1\n",
        "      self.index2token[self.num_tokens] = token\n",
        "      self.num_tokens += 1\n",
        "    else:\n",
        "      self.token2count[token] += 1\n",
        "\n",
        "  def add_chunk(self, chunk):\n",
        "    chunk_len = 0\n",
        "    for token in chunk.split(' '):\n",
        "      chunk_len += 1\n",
        "      self.add_token(token)\n",
        "    if chunk_len > self.longest_chunk:\n",
        "      self.longest_chunk = chunk_len\n",
        "    self.num_chunks +=1\n",
        "  \n",
        "  def to_token(self, index):\n",
        "    return self.index2token[index]\n",
        "  \n",
        "  def to_index(self, token):\n",
        "    return self.token2index[token]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOmUlwHnxHT",
        "colab_type": "text"
      },
      "source": [
        "## Fç tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p9A0cugo_t3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a2fc2a5d-bff4-489a-9d38-40f2ee6d41f5"
      },
      "source": [
        "def tokenizer(corpus, vocab):\n",
        "  tokenizer_word, tokenizer_encoded =[],[]\n",
        "  for chunk in corpus:\n",
        "    word_list  = text_cleaner(chunk)\n",
        "    token_list = [vocab.to_index(w) for w in word_list] \n",
        "    tokenizer_word.append(word_list)\n",
        "    tokenizer_encoded.append(token_list)\n",
        "  return tokenizer_word, tokenizer_encoded\n",
        "\n",
        "##########\n",
        "# Testing\n",
        "########## \n",
        "\n",
        "voc = Vocab(build_vocab(corpus)) # Tokenizer class\n",
        "for sent in vocab: # creating the vocab\n",
        "  voc.add_chunk(sent)\n",
        "\n",
        "tokenized_word, tokenized_encoded = tokenizer(corpus, voc)\n",
        "print(f'My tokenizer solution: {tokenized_word}')\n",
        "print(f'{tokenized_encoded}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My tokenizer solution: [['this', 'is', 'the', 'first', 'document'], ['this', 'document', 'is', 'the', 'second', 'document'], ['and', 'this', 'is', 'the', 'third', 'one'], ['is', 'this', 'the', 'first', 'document']]\n",
            "[[8, 3, 6, 2, 1], [8, 1, 3, 6, 5, 1], [0, 8, 3, 6, 7, 4], [3, 8, 6, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhRjfqFlIDD6",
        "colab_type": "text"
      },
      "source": [
        "## Comparando com o Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V47OC8nM-7n0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "338e0c2d-5b1c-417c-9328-6fb4b609c586"
      },
      "source": [
        "skl_tknzd = [cv.build_tokenizer()(chunk) for chunk in corpus]\n",
        "print(f'SKLearn: {skl_tknzd}')\n",
        "\n",
        "for chunk in skl_tknzd:\n",
        "    for _, token in enumerate(chunk):\n",
        "        chunk[_] = names.index(token.lower())\n",
        "print(f'{skl_tknzd}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SKLearn: [['This', 'is', 'the', 'first', 'document'], ['This', 'document', 'is', 'the', 'second', 'document'], ['And', 'this', 'is', 'the', 'third', 'one'], ['Is', 'this', 'the', 'first', 'document']]\n",
            "[[8, 3, 6, 2, 1], [8, 1, 3, 6, 5, 1], [0, 8, 3, 6, 7, 4], [3, 8, 6, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NzHkdBqrPTy",
        "colab_type": "text"
      },
      "source": [
        "# Fç feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEGd6rZb6QiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bf05db42-f50a-4fdb-f2a7-153e2f0f0912"
      },
      "source": [
        "def feature(tokenized_encoded):\n",
        "  allbags = []\n",
        "  max_lenght = max([max(v) for v in tokenized_encoded])\n",
        "  for chunk in tokenized_encoded:\n",
        "    bag_vector = np.zeros(max_lenght+1)\n",
        "    for i, position in enumerate(chunk):\n",
        "      bag_vector[position] += 1  \n",
        "    allbags.append(bag_vector)\n",
        "  return np.asarray(allbags).astype(int)\n",
        "\n",
        "##########\n",
        "# Testing\n",
        "##########\n",
        "\n",
        "print(f'Corpus: {corpus}')\n",
        "doc_term = feature(tokenized_encoded)\n",
        "print(f'My doc_term solution:\\n{doc_term}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus: ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
            "My doc_term solution:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK9OHaAx0h3k",
        "colab_type": "text"
      },
      "source": [
        "## Comparando com o Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-s-zIS3XrLdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cd833443-2dce-4c98-c394-ee06adbc3270"
      },
      "source": [
        "print(f'Corpus: {corpus}')\n",
        "X = cv.fit_transform(corpus)\n",
        "print(f'SKLearn:\\n{X.toarray()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus: ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
            "SKLearn:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOpQ7mWYaCWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20f4534f-986f-463e-a9b0-d757fed28f2d"
      },
      "source": [
        "# Comparação final \n",
        "(doc_term == X.toarray()).all()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5OIxrCqaIjV",
        "colab_type": "text"
      },
      "source": [
        "## Para o IMDB com 1000 amostras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH5LBkVLad-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5a222969-0806-4c0a-b575-5bc1a07c3e09"
      },
      "source": [
        "#################\n",
        "# 1o build vocab\n",
        "#################\n",
        "vocab = build_vocab(df.loc[:1000, 'text',].tolist())\n",
        "\n",
        "##########################\n",
        "# 2o instance Vocab class\n",
        "##########################\n",
        "voc = Vocab(vocab) \n",
        "for sent in vocab: \n",
        "  voc.add_chunk(sent)\n",
        "\n",
        "##########################\n",
        "# 3o tokenize the samples\n",
        "##########################\n",
        "tokenized_word, tokenized_encoded = tokenizer(df.loc[:1000, 'text',].tolist(), voc)\n",
        "\n",
        "####################\n",
        "# 4o Build doc_term\n",
        "####################\n",
        "doc_term = feature(tokenized_encoded)\n",
        "print(f'My doc_term solution:\\n{doc_term}')\n",
        "print(f'Shape:\\n{doc_term.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My doc_term solution:\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [3 0 0 ... 0 0 0]]\n",
            "Shape:\n",
            "(1000, 20077)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVM2q_BkaneD",
        "colab_type": "text"
      },
      "source": [
        "### Com o Sklearn IMDB (IMDB 1000 samples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVxCcamTCtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "761a2f05-edf6-4f0f-9470-344ffbf0e681"
      },
      "source": [
        "X = cv.fit_transform(df.loc[:1000, 'text',].tolist())\n",
        "print(f'SKLearn:\\n{X.toarray()}')\n",
        "print(f'Shape:\\n{X.toarray().shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SKLearn:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Shape:\n",
            "(1000, 18668)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmjcOVm6CNCN",
        "colab_type": "text"
      },
      "source": [
        "### Conclusão\n",
        "- Minha função que processa o texto é muito mais simples do que a do SKlearn, desse\n",
        "fato, é necessário um melhor conjunto de regras para que os shapes fiquem iguais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8_EFvxXyD9",
        "colab_type": "text"
      },
      "source": [
        "### fim do notebook\n"
      ]
    }
  ]
}