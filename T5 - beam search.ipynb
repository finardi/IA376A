{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Paulo Finardi [Sem: 10].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1cdrcbhN4lhz-Zw_xkdyck6jdJ8gdPDyY",
      "authorship_tag": "ABX9TyPFmAXtBt7FMMTj+dVM070j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/IA376A/blob/master/T5%20-%20beam%20search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eZXCeC_h4tvF"
      },
      "source": [
        "<style type=\"text/css\">\n",
        "@media print { body { -webkit-print-color-adjust: exact; } }\n",
        "</style>\n",
        "\n",
        "\n",
        "\n",
        "# <span style=\"color:orange\"> Paulo Finardi </span>\n",
        "<span style=\"color:purple\"> - Semana 10 </span>\n",
        "\n",
        "Implementação de decodificadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AU7R-cSmRHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "aa0ccc14-9e89-4e73-d090-4de2cf0eab9b"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 14 01:11:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8     8W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mXaMmG4cb-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q sacrebleu\n",
        "! pip install -q transformers\n",
        "! pip install --q unidecode\n",
        "! pip install -q pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inw62fUG8kJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "775a6428-19ab-4639-f311-3bce85af9134"
      },
      "source": [
        "# Basics\n",
        "import os\n",
        "import gzip\n",
        "import random\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import unidecode\n",
        "\n",
        "# PyTorch\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Dataset e PyTorch Lightning\n",
        "import sacrebleu\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# Transformers\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "#Typing\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Tuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.5.0+cu101 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.2.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJlZDb1VY29r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27866b9e-cdf9-4ecf-834f-f3936c58fc17"
      },
      "source": [
        "manual_seed = 0\n",
        "def deterministic(rep=True):\n",
        "    if rep:\n",
        "        np.random.seed(manual_seed)\n",
        "        torch.manual_seed(manual_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(manual_seed)\n",
        "            torch.cuda.manual_seed_all(manual_seed)\n",
        "        torch.backends.cudnn.enabled = False \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        print(f'Deterministic experiment, seed: {manual_seed}')\n",
        "    else:\n",
        "        print('Random experiment')\n",
        "deterministic()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deterministic experiment, seed: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Co8U6O4Gl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a61c35a4-2af0-4040-a512-80145ddfab60"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hf1K6X5KheOG"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i1KmnsxmheOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cd504b4d-34dd-46e6-eda4-1e0a7ab2e806"
      },
      "source": [
        "! wget -nc https://storage.googleapis.com/neuralresearcher_data/unicamp/ia376e_2020s1/paracrawl_enpt_train.tsv.gz\n",
        "! wget -nc https://storage.googleapis.com/neuralresearcher_data/unicamp/ia376e_2020s1/paracrawl_enpt_test.tsv.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘paracrawl_enpt_train.tsv.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘paracrawl_enpt_test.tsv.gz’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_text_pairs(path):\n",
        "    text_pairs = []\n",
        "    for line in gzip.open(path, mode='rt'):\n",
        "        text_pairs.append(line.strip().split('\\t'))\n",
        "    return text_pairs\n",
        "\n",
        "x_train_ = load_text_pairs('paracrawl_enpt_train.tsv.gz')\n",
        "x_test  = load_text_pairs('paracrawl_enpt_test.tsv.gz')\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/val.\n",
        "random.shuffle(x_train_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzoxiwMMZl2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2ec0ab2-a44d-451d-c447-471a5fba9f01"
      },
      "source": [
        "# conj. treino = 20k amostras\n",
        "# conj. valid  = 2k amostras\n",
        "\n",
        "split = 20_000\n",
        "x_train = x_train_[:split]\n",
        "x_val   = x_train_[split: split+ 2_000]  \n",
        "len(x_train), len(x_val), len(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 2000, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptNt_x5MYJ6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8a3b4999-fa48-449b-ce46-3588561815fc"
      },
      "source": [
        "size_model = 't5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(size_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpgCufsDCxAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_max_length, target_max_length = 256, 256\n",
        "\n",
        "class MyDS(Dataset):\n",
        "    def __init__(self, text_pairs, tokenizer, source_max_length, target_max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_pairs = text_pairs\n",
        "        self.source_max_length = source_max_length\n",
        "        self.target_max_length = target_max_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.text_pairs)\n",
        "\n",
        "    def add_prefix(self,text,source_language='English',target_language='Portuguese'):\n",
        "        prefix_text = f'translate {source_language} to {target_language}: '\n",
        "        return prefix_text+text\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        source, target = self.text_pairs[idx]\n",
        "\n",
        "        source_tokenized = \\\n",
        "        self.tokenizer.encode_plus(self.add_prefix(f'{unidecode.unidecode(source)} {self.tokenizer.eos_token}'),\n",
        "                                max_length=self.source_max_length,\n",
        "                                pad_to_max_length=True,\n",
        "                                return_tensors='pt')\n",
        "        target_tokenized = \\\n",
        "        self.tokenizer.encode_plus(f'{unidecode.unidecode(target)} {self.tokenizer.eos_token}',\n",
        "                                max_length=self.target_max_length,\n",
        "                                pad_to_max_length=True,\n",
        "                                return_tensors='pt')\n",
        "\n",
        "        source_token_ids = source_tokenized['input_ids'].squeeze()\n",
        "        source_mask = source_tokenized['attention_mask'].squeeze()\n",
        "        original_source = source\n",
        "        \n",
        "        target_token_ids = target_tokenized['input_ids'].squeeze()\n",
        "        target_mask = target_tokenized['attention_mask'].squeeze()\n",
        "        original_target = target\n",
        "\n",
        "        return (original_source, source_token_ids, source_mask, \n",
        "                original_target, target_token_ids, target_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloyt0tIwIiD",
        "colab_type": "text"
      },
      "source": [
        "## Testando o Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoKiQXCvwGrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5b709df8-fa80-4994-a010-93c665034062"
      },
      "source": [
        "text_pairs = [('we like pizza', 'eu gosto de pizza')]\n",
        "dataset_debug = MyDS(text_pairs, tokenizer, 10,10)\n",
        "\n",
        "dataloader_debug = DataLoader(dataset_debug, batch_size=10, shuffle=True, num_workers=0)\n",
        "\n",
        "x, x_ids, x_mask, y, y_ids, y_mask = next(iter(dataloader_debug))\n",
        "print('source_token_ids:\\n', x_ids)\n",
        "print('source_mask:\\n', x_mask)\n",
        "print('target_token_ids:\\n', y_ids)\n",
        "print('target_mask:\\n', y_mask)\n",
        "\n",
        "print('source_token_ids.shape:', x_ids.shape)\n",
        "print('source_mask.shape:', x_mask.shape)\n",
        "print('target_token_ids.shape:', y_ids.shape)\n",
        "print('target_mask.shape:', y_mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source_token_ids:\n",
            " tensor([[13959,  1566,    12, 21076,    10,    62,   114,  6871,     1,     0]])\n",
            "source_mask:\n",
            " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
            "target_token_ids:\n",
            " tensor([[   3,   15,   76,  281,    7,  235,   20, 6871,    1,    0]])\n",
            "target_mask:\n",
            " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
            "source_token_ids.shape: torch.Size([1, 10])\n",
            "source_mask.shape: torch.Size([1, 10])\n",
            "target_token_ids.shape: torch.Size([1, 10])\n",
            "target_mask.shape: torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBptbyTvXBhC",
        "colab_type": "text"
      },
      "source": [
        "### Datasets e Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2_Fcs0VXD5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29d5c3a8-50fd-4a3a-e7de-21f1a94e5c11"
      },
      "source": [
        "x_max_length = 256\n",
        "y_max_length = 256\n",
        "batch_size   = 4\n",
        "\n",
        "ds_train = MyDS(x_train, tokenizer, x_max_length, y_max_length)\n",
        "ds_val   = MyDS(x_val, tokenizer, x_max_length, y_max_length)\n",
        "ds_test  = MyDS(x_test[:1000], tokenizer, x_max_length, y_max_length)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(ds_train,\n",
        "                        batch_size=batch_size,\n",
        "                        num_workers=4,\n",
        "                        pin_memory=True),\n",
        "    'val':   DataLoader(ds_val,\n",
        "                        batch_size=batch_size,\n",
        "                        num_workers=4,\n",
        "                        pin_memory=False),\n",
        "    'test':  DataLoader(ds_test,\n",
        "                        batch_size=batch_size,\n",
        "                        num_workers=4,\n",
        "                        pin_memory=False),\n",
        "               }\n",
        "\n",
        "# sanity check\n",
        "dl_sizes = {x: len(dataloaders[x]) for x in dataloaders.keys()}\n",
        "dl_sizes "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': 250, 'train': 5000, 'val': 500}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS_N13ITX9yf",
        "colab_type": "text"
      },
      "source": [
        "# Implementação Top_p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWLo7rAcpkov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Top_p(torch.nn.Module):\n",
        "    def __init__(self, p=0.8, min_samples=1):\n",
        "        super(Top_p, self).__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(size_model)\n",
        "        self.p = p\n",
        "        self.min_samples = min_samples\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.model.generate(*args, **kwargs)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.model(*args, **kwargs)\n",
        "\n",
        "# Implementação inspirada no cód. do Israel\n",
        "    @torch.no_grad()\n",
        "    def topp_decode(self, input_ids, attention_mask=None, max_seq_len=32, **decode_kwargs):\n",
        "        self.eval()\n",
        "        batch_size = input_ids.shape[0]\n",
        "        device = input_ids.device\n",
        "        eos_token_id = self.model.config.eos_token_id\n",
        "        \n",
        "        encoder_outs   = self.model.encoder(input_ids, attention_mask)\n",
        "        decoder_inputs = self.model.config.decoder_start_token_id *\\\n",
        "                         torch.ones(batch_size, 1, dtype=torch.long, device=device)\n",
        "        \n",
        "        for i in range(max_seq_len):\n",
        "            logits, _, _ = self(encoder_outputs=encoder_outs, \n",
        "                                decoder_input_ids=decoder_inputs, \n",
        "                                attention_mask=attention_mask)\n",
        "         \n",
        "            probs = torch.softmax(logits, dim=-1)[:,-1,:]\n",
        "            sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)\n",
        "            probs_cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
        "            to_zero = (probs_cumsum > self.p)[:,self.min_samples:]\n",
        "            sorted_probs[:,self.min_samples:][to_zero] = 0\n",
        "            selected_indices = torch.multinomial(sorted_probs, num_samples=1)\n",
        "            next_token_ids = indices[:, selected_indices.view(-1)][:,0].unsqueeze(1)\n",
        "\n",
        "            last_token_ids = decoder_inputs[:,-1]          \n",
        "            last_was_eos = last_token_ids == eos_token_id  \n",
        "            next_token_ids[last_was_eos,-1] = eos_token_id \n",
        "            all_eos = (next_token_ids == eos_token_id).sum().item() == batch_size\n",
        "            \n",
        "            decoder_inputs = torch.cat((decoder_inputs, next_token_ids), dim=-1) \n",
        "            if all_eos == batch_size:\n",
        "                break\n",
        "        return decoder_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhsZBqgwX6Hc",
        "colab_type": "text"
      },
      "source": [
        "# Implementação Top_k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYOyysSmZ_bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Top_k(torch.nn.Module):\n",
        "    def __init__(self, k=60):\n",
        "        super(Top_k, self, ).__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(size_model)\n",
        "        self.k = k\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.model.generate(*args, **kwargs)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.model(*args, **kwargs)\n",
        "\n",
        "# Implementação inspirada no cód. do Israel\n",
        "    @torch.no_grad()\n",
        "    def topk_decode(self, input_ids, attention_mask=None, max_seq_len=32, **decode_kwargs):\n",
        "        self.eval()\n",
        "        batch_size = input_ids.shape[0]\n",
        "        device = input_ids.device\n",
        "        eos_token_id = self.model.config.eos_token_id\n",
        "        \n",
        "        encoder_outs   = self.model.encoder(input_ids, attention_mask)\n",
        "        decoder_inputs = self.model.config.decoder_start_token_id *\\\n",
        "                         torch.ones(batch_size, 1, dtype=torch.long, device=device)\n",
        "        \n",
        "        for i in range(max_seq_len):\n",
        "            logits, _, _ = self(encoder_outputs=encoder_outs, \n",
        "                                decoder_input_ids=decoder_inputs, \n",
        "                                attention_mask=attention_mask)\n",
        "            \n",
        "            topk_probs, topk_indices = torch.softmax(logits, dim=-1)[:,-1,:].topk(self.k, dim=-1, sorted=True)\n",
        "            selected_indices = torch.multinomial(topk_probs, num_samples=1)\n",
        "            next_token_ids = topk_indices[:, selected_indices.view(-1)][:,0].unsqueeze(1)\n",
        "\n",
        "            last_token_ids = decoder_inputs[:,-1]          # take last predicted token\n",
        "            last_was_eos = last_token_ids == eos_token_id  # checks if eos was predicted\n",
        "            next_token_ids[last_was_eos,-1] = eos_token_id # if the last was eos then next is eos\n",
        "            all_eos = (next_token_ids == eos_token_id).sum().item() == batch_size # check if all new tokens are eos\n",
        "            \n",
        "            decoder_inputs = torch.cat((decoder_inputs, next_token_ids), dim=-1) # concat\n",
        "            if all_eos == batch_size:\n",
        "                break\n",
        "        return decoder_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4yoOCBXPEJ",
        "colab_type": "text"
      },
      "source": [
        "# Testando Top_k\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtmiZPIQV5Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Top_k(k=80).to(device)\n",
        "\n",
        "ckpt = torch.load('/content/drive/My Drive/Colab Notebooks/Semana10/epoch=3.ckpt', map_location=device)\n",
        "ckpt.keys()\n",
        "model.load_state_dict(ckpt['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpJHBdGJBIk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "359fd1d3-e961-42df-f3cc-d50677b3727f"
      },
      "source": [
        "model = Top_k(k=80).to(device)\n",
        "\n",
        "ckpt = torch.load('/content/drive/My Drive/Colab Notebooks/Semana10/epoch=3.ckpt', map_location=device)\n",
        "ckpt.keys()\n",
        "model.load_state_dict(ckpt['state_dict'])\n",
        "\n",
        "_, x, mask, y, y_token_ids, y_mask = next(iter(dataloaders['train']))\n",
        "max_seq_len = 128\n",
        "\n",
        "target_token_ids = model.topk_decode(x.to(device), mask.to(device), max_seq_len=max_seq_len)\n",
        "for seq in target_token_ids:\n",
        "    print(tokenizer.decode(seq), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serviços de jardinagem em Cauca (Santa Cruz, Bol ⁇ via) \n",
            "\n",
            "Com vistas espetacular-se do mar e da praia em todos os quartos, desfrute de uma maravilhosa pôr do sol em frente ao mar do seu grande terraço com mais de 45 m2. \n",
            "\n",
            "Farmacêutica em Modegua, Peru \n",
            "\n",
            "Cl ⁇ nicas e hospitalose em Texas \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8B5pIZhT-Uu",
        "colab_type": "text"
      },
      "source": [
        "# Testando Nucleus - top_p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CHr_dpJXV4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "model = Top_p(p=0.95, min_samples=1).to(device)\n",
        "ckpt = torch.load('/content/drive/My Drive/Colab Notebooks/Semana10/epoch=3.ckpt', map_location=device)\n",
        "ckpt.keys()\n",
        "model.load_state_dict(ckpt['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-g2h6dr2JYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "59f29b8f-2c8a-46a8-cd77-5ec4e8e0255f"
      },
      "source": [
        "target_token_ids = model.topp_decode(x.to(device), mask.to(device), max_seq_len=max_seq_len)\n",
        "for seq in target_token_ids:\n",
        "    print(tokenizer.decode(seq), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serviços de jardinagem em Pedro Rubio (Santa Cruz, Bol ⁇ via) \n",
            "\n",
            "Com vistas espetacularionais do mar e da praia em todos os quartos, desfrute de uma maravilhosa pôr do sol em frente ao mar do seu grande terraço com mais de 45 m2. \n",
            "\n",
            "Farmacêutica em Moyuria, Peru \n",
            "\n",
            "Cl ⁇ nicas e hospitaconismo em Texas \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5yUuMVfs92w",
        "colab_type": "text"
      },
      "source": [
        "# Comparando com o Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jg-hZEktbvnr",
        "colab": {}
      },
      "source": [
        "class T5Finetuner(pl.LightningModule):\n",
        "    def __init__(self, tokenizer, dataloader):\n",
        "        super(T5Finetuner, self).__init__()\n",
        "\n",
        "        self.model      = T5ForConditionalGeneration.from_pretrained(size_model)\n",
        "        self.dataloader = dataloader\n",
        "        self.tokenizer  = tokenizer\n",
        "\n",
        "    def forward(self, x_token_ids, x_mask, y_token_ids=None, y_mask=None):\n",
        "        if self.training:\n",
        "            outputs = self.model.forward(input_ids = x_token_ids, attention_mask = x_mask,\n",
        "                                         lm_labels  = y_token_ids)\n",
        "            return outputs[0] \n",
        "        else:\n",
        "            predicted_token_ids = self.model.generate(input_ids = x_token_ids, attention_mask = x_mask,\n",
        "                                                      max_length=128)\n",
        "            return predicted_token_ids\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad],lr=5e-3)\n",
        "\n",
        "    def decode_token_ids(self, x_token_ids):\n",
        "        translation = self.tokenizer.decode(x_token_ids,\n",
        "                                            skip_special_tokens=True,\n",
        "                                            clean_up_tokenization_spaces=False)\n",
        "        return translation\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        x_token_ids, x_mask, y_token_ids, y_mask, _, _ = batch\n",
        "        loss = self(x_token_ids, x_mask, y_token_ids, y_mask)\n",
        "        \n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        progress_bar     = {'gpu_usage': gpu_usage()}\n",
        "        return {'loss': loss, 'log': tensorboard_logs, 'progress_bar': progress_bar}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        x_token_ids, x_mask, y_token_ids, y_mask, x, y = batch\n",
        "        preds_token_ids  = self(x_token_ids, x_mask)\n",
        "        preds = [self.decode_token_ids(token_ids) for token_ids in preds_token_ids]\n",
        "        bleu_score       = sacrebleu.corpus_bleu(preds, [y]).score\n",
        "        tensorboard_logs = {'val_bleu': bleu_score}\n",
        "        progress_bar     = {'gpu_usage': gpu_usage()}\n",
        "        return {'val_bleu': bleu_score, 'progress_bar': progress_bar, 'log':tensorboard_logs}\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        x_token_ids, x_mask, y_token_ids, y_mask, x, y = batch\n",
        "        preds_token_ids = self(x_token_ids, x_mask)\n",
        "        preds = [self.decode_token_ids(token_ids) for token_ids in preds_token_ids]\n",
        "        bleu_score   = sacrebleu.corpus_bleu(preds, [y]).score\n",
        "        progress_bar = {'gpu_usage': gpu_usage()}\n",
        "        return {'test_bleu': bleu_score, 'progress_bar': progress_bar}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        bleu_score       = sum([x['val_bleu'] for x in outputs]) / len(outputs)\n",
        "        tensorboard_logs = {'avg_val_bleu': bleu_score}\n",
        "        return {'avg_val_bleu': bleu_score, 'progress_bar': tensorboard_logs, 'log': tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_loss         = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'train_loss': avg_loss}\n",
        "        return {'log': tensorboard_logs}\n",
        "        \n",
        "    def test_epoch_end(self, outputs):\n",
        "        bleu_av          = sum([x['test_bleu'] for x in outputs]) / len(outputs)\n",
        "        tensorboard_logs = {'avg_test_bleu': bleu_av}\n",
        "        return {'avg_test_bleu': bleu_av, 'progress_bar': tensorboard_logs}\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self.dataloader['train']\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self.dataloader['val']\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return self.dataloader['test']\n",
        "\n",
        "model = T5Finetuner(tokenizer, dataloaders)\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEJnhapg3mKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d2854f1-e7b9-4cab-bb8a-788f251b749e"
      },
      "source": [
        "trainer = pl.Trainer(gpus=1,\n",
        "                     max_epochs=2,\n",
        "                     progress_bar_refresh_rate=60,\n",
        "                     accumulate_grad_batches=8,\n",
        "                     resume_from_checkpoint='/content/drive/My Drive/Colab Notebooks/Semana10/epoch=3.ckpt')\n",
        "\n",
        "model = T5Finetuner(tokenizer, dataloaders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:lightning:GPU available: True, used: True\n",
            "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json from cache at /root/.cache/torch/transformers/26561bc9e840d8945f475d0d4c4b9df32025eadd79894b867b570cb1d09e67a9.3817cc1260a6b941b17af62b4f2a942b9825f209d8e2eed99e79e96f85f59aab\n",
            "INFO:transformers.configuration_utils:Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/t5-small-pytorch_model.bin from cache at /root/.cache/torch/transformers/9b662cba85524bef76fff5eb77d767407ac36f3fe492869331c011efd2b3a082.04cbd562eb01c8cec40cf5fefcc83823e6ba146bd2a758202a770beb7d80bb5d\n",
            "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in T5ForConditionalGeneration: ['encoder.block.0.layer.0.layer_norm.bias', 'encoder.block.0.layer.1.layer_norm.bias', 'encoder.block.1.layer.0.layer_norm.bias', 'encoder.block.1.layer.1.layer_norm.bias', 'encoder.block.2.layer.0.layer_norm.bias', 'encoder.block.2.layer.1.layer_norm.bias', 'encoder.block.3.layer.0.layer_norm.bias', 'encoder.block.3.layer.1.layer_norm.bias', 'encoder.block.4.layer.0.layer_norm.bias', 'encoder.block.4.layer.1.layer_norm.bias', 'encoder.block.5.layer.0.layer_norm.bias', 'encoder.block.5.layer.1.layer_norm.bias', 'encoder.final_layer_norm.bias', 'decoder.block.0.layer.0.layer_norm.bias', 'decoder.block.0.layer.1.layer_norm.bias', 'decoder.block.0.layer.2.layer_norm.bias', 'decoder.block.1.layer.0.layer_norm.bias', 'decoder.block.1.layer.1.layer_norm.bias', 'decoder.block.1.layer.2.layer_norm.bias', 'decoder.block.2.layer.0.layer_norm.bias', 'decoder.block.2.layer.1.layer_norm.bias', 'decoder.block.2.layer.2.layer_norm.bias', 'decoder.block.3.layer.0.layer_norm.bias', 'decoder.block.3.layer.1.layer_norm.bias', 'decoder.block.3.layer.2.layer_norm.bias', 'decoder.block.4.layer.0.layer_norm.bias', 'decoder.block.4.layer.1.layer_norm.bias', 'decoder.block.4.layer.2.layer_norm.bias', 'decoder.block.5.layer.0.layer_norm.bias', 'decoder.block.5.layer.1.layer_norm.bias', 'decoder.block.5.layer.2.layer_norm.bias', 'decoder.final_layer_norm.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vQz-Wnrow9E8"
      },
      "source": [
        "# Comparando Top K"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l-n8VfY92mIT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "bd004dae0b564b879bfa72e88fe829d3",
            "fd695ba070ad4653b6024f3a5393f634",
            "d310fb6e408b4fa7a763d234562471b9",
            "226a53ca1c5c451d8dc7b4ace6f02bd0",
            "a9ea10d7400247ea930a7fa81a704f79",
            "e22b482030ad4c549f05b872a9b0e1b5",
            "fb60bcafaf8c490bb43fb9718a7e85d5",
            "3d4ffae0dd64483f917a47a20e71cdde"
          ]
        },
        "outputId": "2b729ed5-434f-47fa-b12b-38440b7b18f5"
      },
      "source": [
        "top_k = Top_k(k=10).to(device)\n",
        "ckpt = torch.load('/content/drive/My Drive/Colab Notebooks/Semana10/epoch=3.ckpt', map_location=device)\n",
        "top_k.load_state_dict(ckpt['state_dict'])\n",
        "top_k.topk_decode(x.to(device), mask.to(device), max_seq_len=128)\n",
        "trainer.test(top_k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd004dae0b564b879bfa72e88fe829d3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.', '1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto', 'No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.', 'e tem o objetivo de viabilizar a perfuração de dois novos furos no ocidente da citada península.']\n",
            "['E desta maneira, a vida civil da nacao mature, possivel de todos os cidadaos de saber as frutas de tolerancia verdadeira e de respecto reciproco.', '1999 XIII. Winnipeg, Canada', 'No misterio de Novem, a luz de Cristo brilha na terra, spreading, sem como fora, em cercos centros.', 'faz vivavel perfurar dois novos escadas na ocidental do aquilo.']\n",
            "--------------------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "{'test_bleu': 14.378540709075402}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T8oGAW54w9FR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "3f49eb025e6542ec8d5c21edb1958140",
            "62025b857efe49018e9f5306f2b3ec38",
            "4f4a9059af85411d9e564a5702f61ed5",
            "59b8ee812d4e41cf8548beee350da073",
            "f97528a05b0246fba1f7e59a7f06fe86",
            "816beb42f84c4771b26f83d74db071e2",
            "4dbd07f4f4324f09908ad1b495f8d429",
            "f140aabf9b7e49ce880897932fba7197"
          ]
        },
        "outputId": "0d25aa0b-1901-4344-8149-c62ef180d020"
      },
      "source": [
        "# Hugging Face\n",
        "deterministic()\n",
        "model.generate(x, mask, max_length=max_len, do_sample=True, top_k=topk)\n",
        "trainer.test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f49eb025e6542ec8d5c21edb1958140",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.', '1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto', 'No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.', 'e tem o objetivo de viabilizar a perfuração de dois novos furos no ocidente da citada península.']\n",
            "['E desta maneira, a vida civil da nacao mature, possivel de todos os cidadaos de saber as frutas de tolerancia verdadeira e de respecto reciproco.', '1999 XIII. Winnipeg, Canada', 'No misterio de Novem, a luz de Cristo brilha na terra, spreading, sem como fora, em cercos centros.', 'faz vivavel perfurar dois novos escadas na ocidental do aquilo.']\n",
            "--------------------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "{'test_bleu': 14.399085808196498}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xpXv498Y1mqR"
      },
      "source": [
        "# Comparando Nucleus Sampling - Top-p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EQd7YYjE1mqj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "ddc0aa06bd39411ab65450e57825f3f5",
            "457123a9dcc44428add9ace2a990cb93",
            "c55b82efddd5446e853f1faf64266af7",
            "d4596a2d8deb400097b4b533957a619e",
            "21a0899f17b94ab5bcdfc82cbbaf4126",
            "bd2c9dd8c21f4f97a9c13d89d2a630a2",
            "c4ada6592a234aa98074322c27b3f9ba",
            "f1404c50e3874264b8ed6435f4a9efc0"
          ]
        },
        "outputId": "a7733392-6c7c-42bd-ba5c-92e81c3e868d"
      },
      "source": [
        "top_p = Top_p(p=0.95, min_samples=1).to(device)\n",
        "ckpt = torch.load('/content/drive/My Drive/Colab Notebooks/Semana10/epoch=3.ckpt', map_location=device)\n",
        "top_p.load_state_dict(ckpt['state_dict'])\n",
        "top_p.topp_decode(x.to(device), mask.to(device), max_seq_len=128)\n",
        "trainer.test(top_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddc0aa06bd39411ab65450e57825f3f5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.', '1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto', 'No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.', 'e tem o objetivo de viabilizar a perfuração de dois novos furos no ocidente da citada península.']\n",
            "['E desta maneira, a vida civil da nacao mature, possivel de todos os cidadaos de saber as frutas de tolerancia verdadeira e de respecto reciproco.', '1999 XIII. Winnipeg, Canada', 'No misterio de Novem, a luz de Cristo brilha na terra, spreading, sem como fora, em cercos centros.', 'faz vivavel perfurar dois novos foras no estado na dois ocidentais do qualquer presido.']\n",
            "--------------------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "{'test_bleu': 14.819372391368573}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TR-zjPWA3Mf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "1c33d53311864da4b6801084068d81db",
            "7a91a01ae9024b168d54ad59dace08f6",
            "93fa16b739d64ab5a7f28217ba3a85b1",
            "bf111018e0c2497ca76d2e75e81c4842",
            "877625a53f404272bea3abef1694bb54",
            "45748c73ae4040ae8c7e4c6bc245d204",
            "ee133230dd764a6ead055248cb2525b3",
            "678ab5c219114fb3b86a56f06d5a5531"
          ]
        },
        "outputId": "b45ed683-12d9-4584-9da5-621f77e18f9f"
      },
      "source": [
        "# Hugging Face\n",
        "deterministic()\n",
        "model.generate(x, mask, max_length=max_len, do_sample=True, top_p=topp)\n",
        "trainer.test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c33d53311864da4b6801084068d81db",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.', '1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto', 'No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.', 'e tem o objetivo de viabilizar a perfuração de dois novos furos no ocidente da citada península.']\n",
            "['E desta forma, a vida civil de uma nacao matura, perfeito a que todos os cidadaos servem a obras de lămurrio verdadeira toleracao e de respecto mutual.', '1999 XIII. Winnipeg, Canada', 'No misterio de Novembro, a luz de Cristo parva na terra, immerndo, como estava, em cercos centricos.', 'fazendo viual seiar duas novos pei ⁇ res na oeste daquil deda prekaria.']\n",
            "--------------------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "{'test_bleu': 14.997882156408957}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B8Ou8UImuzcx"
      },
      "source": [
        "# <span style=\"color:purple\">Fim do notebook"
      ]
    }
  ]
}